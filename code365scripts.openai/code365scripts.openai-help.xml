<?xml version="1.0" encoding="utf-8"?>
<helpItems schema="maml" xmlns="http://msh">
  <command:command xmlns:maml="http://schemas.microsoft.com/maml/2004/10" xmlns:command="http://schemas.microsoft.com/maml/dev/command/2004/10" xmlns:dev="http://schemas.microsoft.com/maml/dev/2004/10" xmlns:MSHelp="http://msdn.microsoft.com/mshelp">
    <command:details>
      <command:name>New-ChatGPTConversation</command:name>
      <command:verb>New</command:verb>
      <command:noun>ChatGPTConversation</command:noun>
      <maml:description>
        <maml:para>Create a new ChatGPT conversation or get a Chat Completion result.(if you specify the prompt parameter)</maml:para>
      </maml:description>
    </command:details>
    <maml:description>
      <maml:para>Create a new ChatGPT conversation, You can chat with the OpenAI service just like chat with a human. You can also get the chat completion result if you specify the prompt parameter.</maml:para>
    </maml:description>
    <command:syntax>
      <command:syntaxItem>
        <maml:name>New-ChatGPTConversation</maml:name>
        <command:parameter required="true" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="ollama">
          <maml:name>local</maml:name>
          <maml:description>
            <maml:para>If you want to use the local LLMs, like the model hosted by ollama, you can use this switch. You can also use "ollama" as the alias.</maml:para>
          </maml:description>
          <dev:type>
            <maml:name>SwitchParameter</maml:name>
            <maml:uri />
          </dev:type>
          <dev:defaultValue>False</dev:defaultValue>
        </command:parameter>
        <command:parameter required="true" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="engine, deployment">
          <maml:name>model</maml:name>
          <maml:description>
            <maml:para>The model to use for this request, you can also set it in environment variable OPENAI_CHAT_MODEL or OPENAI_CHAT_DEPLOYMENT_AZURE if you use Azure OpenAI service. If you use multiple environments, you can use OPENAI_CHAT_DEPLOYMENT_AZURE_&lt;environment&gt; to define the model for each environment. You can use engine or deployment as the alias of this parameter.</maml:para>
          </maml:description>
          <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
          <dev:type>
            <maml:name>String</maml:name>
            <maml:uri />
          </dev:type>
          <dev:defaultValue>None</dev:defaultValue>
        </command:parameter>
        <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
          <maml:name>endpoint</maml:name>
          <maml:description>
            <maml:para>The endpoint to use for this request, you can also set it in environment variable OPENAI_ENDPOINT or OPENAI_ENDPOINT_AZURE if you use Azure OpenAI service. If you use multiple environments, you can use OPENAI_ENDPOINT_AZURE_&lt;environment&gt; to define the endpoint for each environment.</maml:para>
          </maml:description>
          <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
          <dev:type>
            <maml:name>String</maml:name>
            <maml:uri />
          </dev:type>
          <dev:defaultValue>None</dev:defaultValue>
        </command:parameter>
        <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
          <maml:name>system</maml:name>
          <maml:description>
            <maml:para>The system prompt, this is a string, you can use it to define the role you want it be, for example, "You are a chatbot, please answer the user's question according to the user's language." If you provide a file path to this parameter, we will read the file as the system prompt. You can also specify a url to this parameter, we will read the url as the system prompt. You can read the prompt from a library (https://github.com/code365opensource/promptlibrary), by use "lib:xxxxx" as the prompt, for example, "lib:fitness".</maml:para>
          </maml:description>
          <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
          <dev:type>
            <maml:name>String</maml:name>
            <maml:uri />
          </dev:type>
          <dev:defaultValue>You are a chatbot, please answer the user's question according to the user's language.</dev:defaultValue>
        </command:parameter>
        <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
          <maml:name>prompt</maml:name>
          <maml:description>
            <maml:para>If you want to get result immediately, you can use this parameter to define the prompt. It will not start the chat conversation. If you provide a file path to this parameter, we will read the file as the prompt. You can also specify a url to this parameter, we will read the url as the prompt. You can read the prompt from a library (https://github.com/code365opensource/promptlibrary), by use "lib:xxxxx" as the prompt, for example, "lib:fitness".</maml:para>
          </maml:description>
          <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
          <dev:type>
            <maml:name>String</maml:name>
            <maml:uri />
          </dev:type>
          <dev:defaultValue>None</dev:defaultValue>
        </command:parameter>
        <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
          <maml:name>config</maml:name>
          <maml:description>
            <maml:para>The dynamic settings for the API call, it can meet all the requirement for each model. please pass a custom object to this parameter, like @{temperature=1;max_tokens=1024}.</maml:para>
          </maml:description>
          <command:parameterValue required="true" variableLength="false">PSObject</command:parameterValue>
          <dev:type>
            <maml:name>PSObject</maml:name>
            <maml:uri />
          </dev:type>
          <dev:defaultValue>None</dev:defaultValue>
        </command:parameter>
        <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="out">
          <maml:name>outFile</maml:name>
          <maml:description>
            <maml:para>If you want to save the result to a file, you can use this parameter to set the file path. You can also use "out" as the alias.</maml:para>
          </maml:description>
          <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
          <dev:type>
            <maml:name>String</maml:name>
            <maml:uri />
          </dev:type>
          <dev:defaultValue>None</dev:defaultValue>
        </command:parameter>
        <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
          <maml:name>json</maml:name>
          <maml:description>
            <maml:para>Send the response in json format.</maml:para>
          </maml:description>
          <dev:type>
            <maml:name>SwitchParameter</maml:name>
            <maml:uri />
          </dev:type>
          <dev:defaultValue>False</dev:defaultValue>
        </command:parameter>
        <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
          <maml:name>context</maml:name>
          <maml:description>
            <maml:para>If you want to pass some dymamic value to the prompt, you can use the context parameter here. It can be anything, you just specify a custom powershell object here. You define the variables in the system prompt or user prompt by using {{you_variable_name}} syntext, and then pass the data to the context parameter, like @{you_variable_name="your value"}. if there are multiple variables, you can use @{variable1="value1";variable2="value2"}.</maml:para>
          </maml:description>
          <command:parameterValue required="true" variableLength="false">PSObject</command:parameterValue>
          <dev:type>
            <maml:name>PSObject</maml:name>
            <maml:uri />
          </dev:type>
          <dev:defaultValue>None</dev:defaultValue>
        </command:parameter>
      </command:syntaxItem>
      <command:syntaxItem>
        <maml:name>New-ChatGPTConversation</maml:name>
        <command:parameter required="true" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
          <maml:name>azure</maml:name>
          <maml:description>
            <maml:para>if you use Azure OpenAI service, you can use this switch.</maml:para>
          </maml:description>
          <dev:type>
            <maml:name>SwitchParameter</maml:name>
            <maml:uri />
          </dev:type>
          <dev:defaultValue>False</dev:defaultValue>
        </command:parameter>
        <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
          <maml:name>api_key</maml:name>
          <maml:description>
            <maml:para>The API key to access OpenAI service, if not specified, the API key will be read from environment variable OPENAI_API_KEY. if you use azure OpenAI service, you can specify the API key by environment variable OPENAI_API_KEY_AZURE or OPENAI_API_KEY_AZURE_&lt;environment&gt;, the &lt;environment&gt; can be any names you want, for example, OPENAI_API_KEY_AZURE_DEV, OPENAI_API_KEY_AZURE_PROD, OPENAI_API_KEY_AZURE_TEST, etc.</maml:para>
          </maml:description>
          <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
          <dev:type>
            <maml:name>String</maml:name>
            <maml:uri />
          </dev:type>
          <dev:defaultValue>None</dev:defaultValue>
        </command:parameter>
        <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="engine, deployment">
          <maml:name>model</maml:name>
          <maml:description>
            <maml:para>The model to use for this request, you can also set it in environment variable OPENAI_CHAT_MODEL or OPENAI_CHAT_DEPLOYMENT_AZURE if you use Azure OpenAI service. If you use multiple environments, you can use OPENAI_CHAT_DEPLOYMENT_AZURE_&lt;environment&gt; to define the model for each environment. You can use engine or deployment as the alias of this parameter.</maml:para>
          </maml:description>
          <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
          <dev:type>
            <maml:name>String</maml:name>
            <maml:uri />
          </dev:type>
          <dev:defaultValue>None</dev:defaultValue>
        </command:parameter>
        <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
          <maml:name>endpoint</maml:name>
          <maml:description>
            <maml:para>The endpoint to use for this request, you can also set it in environment variable OPENAI_ENDPOINT or OPENAI_ENDPOINT_AZURE if you use Azure OpenAI service. If you use multiple environments, you can use OPENAI_ENDPOINT_AZURE_&lt;environment&gt; to define the endpoint for each environment.</maml:para>
          </maml:description>
          <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
          <dev:type>
            <maml:name>String</maml:name>
            <maml:uri />
          </dev:type>
          <dev:defaultValue>None</dev:defaultValue>
        </command:parameter>
        <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
          <maml:name>system</maml:name>
          <maml:description>
            <maml:para>The system prompt, this is a string, you can use it to define the role you want it be, for example, "You are a chatbot, please answer the user's question according to the user's language." If you provide a file path to this parameter, we will read the file as the system prompt. You can also specify a url to this parameter, we will read the url as the system prompt. You can read the prompt from a library (https://github.com/code365opensource/promptlibrary), by use "lib:xxxxx" as the prompt, for example, "lib:fitness".</maml:para>
          </maml:description>
          <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
          <dev:type>
            <maml:name>String</maml:name>
            <maml:uri />
          </dev:type>
          <dev:defaultValue>You are a chatbot, please answer the user's question according to the user's language.</dev:defaultValue>
        </command:parameter>
        <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
          <maml:name>prompt</maml:name>
          <maml:description>
            <maml:para>If you want to get result immediately, you can use this parameter to define the prompt. It will not start the chat conversation. If you provide a file path to this parameter, we will read the file as the prompt. You can also specify a url to this parameter, we will read the url as the prompt. You can read the prompt from a library (https://github.com/code365opensource/promptlibrary), by use "lib:xxxxx" as the prompt, for example, "lib:fitness".</maml:para>
          </maml:description>
          <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
          <dev:type>
            <maml:name>String</maml:name>
            <maml:uri />
          </dev:type>
          <dev:defaultValue>None</dev:defaultValue>
        </command:parameter>
        <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
          <maml:name>config</maml:name>
          <maml:description>
            <maml:para>The dynamic settings for the API call, it can meet all the requirement for each model. please pass a custom object to this parameter, like @{temperature=1;max_tokens=1024}.</maml:para>
          </maml:description>
          <command:parameterValue required="true" variableLength="false">PSObject</command:parameterValue>
          <dev:type>
            <maml:name>PSObject</maml:name>
            <maml:uri />
          </dev:type>
          <dev:defaultValue>None</dev:defaultValue>
        </command:parameter>
        <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="env">
          <maml:name>environment</maml:name>
          <maml:description>
            <maml:para>The environment name, if you use Azure OpenAI service, you can use this parameter to define the environment name, it will be used to get the API key, model and endpoint from environment variable. If the environment is not exist, it will use the default environment.  You can use env as the alias of this parameter.</maml:para>
          </maml:description>
          <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
          <dev:type>
            <maml:name>String</maml:name>
            <maml:uri />
          </dev:type>
          <dev:defaultValue>None</dev:defaultValue>
        </command:parameter>
        <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
          <maml:name>api_version</maml:name>
          <maml:description>
            <maml:para>The api version, if you use Azure OpenAI service, you can use this parameter to define the api version, the default value is 2023-09-01-preview.</maml:para>
          </maml:description>
          <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
          <dev:type>
            <maml:name>String</maml:name>
            <maml:uri />
          </dev:type>
          <dev:defaultValue>2023-09-01-preview</dev:defaultValue>
        </command:parameter>
        <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="out">
          <maml:name>outFile</maml:name>
          <maml:description>
            <maml:para>If you want to save the result to a file, you can use this parameter to set the file path. You can also use "out" as the alias.</maml:para>
          </maml:description>
          <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
          <dev:type>
            <maml:name>String</maml:name>
            <maml:uri />
          </dev:type>
          <dev:defaultValue>None</dev:defaultValue>
        </command:parameter>
        <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
          <maml:name>json</maml:name>
          <maml:description>
            <maml:para>Send the response in json format.</maml:para>
          </maml:description>
          <dev:type>
            <maml:name>SwitchParameter</maml:name>
            <maml:uri />
          </dev:type>
          <dev:defaultValue>False</dev:defaultValue>
        </command:parameter>
        <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
          <maml:name>context</maml:name>
          <maml:description>
            <maml:para>If you want to pass some dymamic value to the prompt, you can use the context parameter here. It can be anything, you just specify a custom powershell object here. You define the variables in the system prompt or user prompt by using {{you_variable_name}} syntext, and then pass the data to the context parameter, like @{you_variable_name="your value"}. if there are multiple variables, you can use @{variable1="value1";variable2="value2"}.</maml:para>
          </maml:description>
          <command:parameterValue required="true" variableLength="false">PSObject</command:parameterValue>
          <dev:type>
            <maml:name>PSObject</maml:name>
            <maml:uri />
          </dev:type>
          <dev:defaultValue>None</dev:defaultValue>
        </command:parameter>
      </command:syntaxItem>
      <command:syntaxItem>
        <maml:name>New-ChatGPTConversation</maml:name>
        <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
          <maml:name>api_key</maml:name>
          <maml:description>
            <maml:para>The API key to access OpenAI service, if not specified, the API key will be read from environment variable OPENAI_API_KEY. if you use azure OpenAI service, you can specify the API key by environment variable OPENAI_API_KEY_AZURE or OPENAI_API_KEY_AZURE_&lt;environment&gt;, the &lt;environment&gt; can be any names you want, for example, OPENAI_API_KEY_AZURE_DEV, OPENAI_API_KEY_AZURE_PROD, OPENAI_API_KEY_AZURE_TEST, etc.</maml:para>
          </maml:description>
          <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
          <dev:type>
            <maml:name>String</maml:name>
            <maml:uri />
          </dev:type>
          <dev:defaultValue>None</dev:defaultValue>
        </command:parameter>
        <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="engine, deployment">
          <maml:name>model</maml:name>
          <maml:description>
            <maml:para>The model to use for this request, you can also set it in environment variable OPENAI_CHAT_MODEL or OPENAI_CHAT_DEPLOYMENT_AZURE if you use Azure OpenAI service. If you use multiple environments, you can use OPENAI_CHAT_DEPLOYMENT_AZURE_&lt;environment&gt; to define the model for each environment. You can use engine or deployment as the alias of this parameter.</maml:para>
          </maml:description>
          <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
          <dev:type>
            <maml:name>String</maml:name>
            <maml:uri />
          </dev:type>
          <dev:defaultValue>None</dev:defaultValue>
        </command:parameter>
        <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
          <maml:name>endpoint</maml:name>
          <maml:description>
            <maml:para>The endpoint to use for this request, you can also set it in environment variable OPENAI_ENDPOINT or OPENAI_ENDPOINT_AZURE if you use Azure OpenAI service. If you use multiple environments, you can use OPENAI_ENDPOINT_AZURE_&lt;environment&gt; to define the endpoint for each environment.</maml:para>
          </maml:description>
          <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
          <dev:type>
            <maml:name>String</maml:name>
            <maml:uri />
          </dev:type>
          <dev:defaultValue>None</dev:defaultValue>
        </command:parameter>
        <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
          <maml:name>system</maml:name>
          <maml:description>
            <maml:para>The system prompt, this is a string, you can use it to define the role you want it be, for example, "You are a chatbot, please answer the user's question according to the user's language." If you provide a file path to this parameter, we will read the file as the system prompt. You can also specify a url to this parameter, we will read the url as the system prompt. You can read the prompt from a library (https://github.com/code365opensource/promptlibrary), by use "lib:xxxxx" as the prompt, for example, "lib:fitness".</maml:para>
          </maml:description>
          <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
          <dev:type>
            <maml:name>String</maml:name>
            <maml:uri />
          </dev:type>
          <dev:defaultValue>You are a chatbot, please answer the user's question according to the user's language.</dev:defaultValue>
        </command:parameter>
        <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
          <maml:name>prompt</maml:name>
          <maml:description>
            <maml:para>If you want to get result immediately, you can use this parameter to define the prompt. It will not start the chat conversation. If you provide a file path to this parameter, we will read the file as the prompt. You can also specify a url to this parameter, we will read the url as the prompt. You can read the prompt from a library (https://github.com/code365opensource/promptlibrary), by use "lib:xxxxx" as the prompt, for example, "lib:fitness".</maml:para>
          </maml:description>
          <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
          <dev:type>
            <maml:name>String</maml:name>
            <maml:uri />
          </dev:type>
          <dev:defaultValue>None</dev:defaultValue>
        </command:parameter>
        <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
          <maml:name>config</maml:name>
          <maml:description>
            <maml:para>The dynamic settings for the API call, it can meet all the requirement for each model. please pass a custom object to this parameter, like @{temperature=1;max_tokens=1024}.</maml:para>
          </maml:description>
          <command:parameterValue required="true" variableLength="false">PSObject</command:parameterValue>
          <dev:type>
            <maml:name>PSObject</maml:name>
            <maml:uri />
          </dev:type>
          <dev:defaultValue>None</dev:defaultValue>
        </command:parameter>
        <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="out">
          <maml:name>outFile</maml:name>
          <maml:description>
            <maml:para>If you want to save the result to a file, you can use this parameter to set the file path. You can also use "out" as the alias.</maml:para>
          </maml:description>
          <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
          <dev:type>
            <maml:name>String</maml:name>
            <maml:uri />
          </dev:type>
          <dev:defaultValue>None</dev:defaultValue>
        </command:parameter>
        <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
          <maml:name>json</maml:name>
          <maml:description>
            <maml:para>Send the response in json format.</maml:para>
          </maml:description>
          <dev:type>
            <maml:name>SwitchParameter</maml:name>
            <maml:uri />
          </dev:type>
          <dev:defaultValue>False</dev:defaultValue>
        </command:parameter>
        <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
          <maml:name>context</maml:name>
          <maml:description>
            <maml:para>If you want to pass some dymamic value to the prompt, you can use the context parameter here. It can be anything, you just specify a custom powershell object here. You define the variables in the system prompt or user prompt by using {{you_variable_name}} syntext, and then pass the data to the context parameter, like @{you_variable_name="your value"}. if there are multiple variables, you can use @{variable1="value1";variable2="value2"}.</maml:para>
          </maml:description>
          <command:parameterValue required="true" variableLength="false">PSObject</command:parameterValue>
          <dev:type>
            <maml:name>PSObject</maml:name>
            <maml:uri />
          </dev:type>
          <dev:defaultValue>None</dev:defaultValue>
        </command:parameter>
      </command:syntaxItem>
    </command:syntax>
    <command:parameters>
      <command:parameter required="true" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="ollama">
        <maml:name>local</maml:name>
        <maml:description>
          <maml:para>If you want to use the local LLMs, like the model hosted by ollama, you can use this switch. You can also use "ollama" as the alias.</maml:para>
        </maml:description>
        <command:parameterValue required="false" variableLength="false">SwitchParameter</command:parameterValue>
        <dev:type>
          <maml:name>SwitchParameter</maml:name>
          <maml:uri />
        </dev:type>
        <dev:defaultValue>False</dev:defaultValue>
      </command:parameter>
      <command:parameter required="true" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
        <maml:name>azure</maml:name>
        <maml:description>
          <maml:para>if you use Azure OpenAI service, you can use this switch.</maml:para>
        </maml:description>
        <command:parameterValue required="false" variableLength="false">SwitchParameter</command:parameterValue>
        <dev:type>
          <maml:name>SwitchParameter</maml:name>
          <maml:uri />
        </dev:type>
        <dev:defaultValue>False</dev:defaultValue>
      </command:parameter>
      <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
        <maml:name>api_key</maml:name>
        <maml:description>
          <maml:para>The API key to access OpenAI service, if not specified, the API key will be read from environment variable OPENAI_API_KEY. if you use azure OpenAI service, you can specify the API key by environment variable OPENAI_API_KEY_AZURE or OPENAI_API_KEY_AZURE_&lt;environment&gt;, the &lt;environment&gt; can be any names you want, for example, OPENAI_API_KEY_AZURE_DEV, OPENAI_API_KEY_AZURE_PROD, OPENAI_API_KEY_AZURE_TEST, etc.</maml:para>
        </maml:description>
        <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
        <dev:type>
          <maml:name>String</maml:name>
          <maml:uri />
        </dev:type>
        <dev:defaultValue>None</dev:defaultValue>
      </command:parameter>
      <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="engine, deployment">
        <maml:name>model</maml:name>
        <maml:description>
          <maml:para>The model to use for this request, you can also set it in environment variable OPENAI_CHAT_MODEL or OPENAI_CHAT_DEPLOYMENT_AZURE if you use Azure OpenAI service. If you use multiple environments, you can use OPENAI_CHAT_DEPLOYMENT_AZURE_&lt;environment&gt; to define the model for each environment. You can use engine or deployment as the alias of this parameter.</maml:para>
        </maml:description>
        <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
        <dev:type>
          <maml:name>String</maml:name>
          <maml:uri />
        </dev:type>
        <dev:defaultValue>None</dev:defaultValue>
      </command:parameter>
      <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
        <maml:name>endpoint</maml:name>
        <maml:description>
          <maml:para>The endpoint to use for this request, you can also set it in environment variable OPENAI_ENDPOINT or OPENAI_ENDPOINT_AZURE if you use Azure OpenAI service. If you use multiple environments, you can use OPENAI_ENDPOINT_AZURE_&lt;environment&gt; to define the endpoint for each environment.</maml:para>
        </maml:description>
        <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
        <dev:type>
          <maml:name>String</maml:name>
          <maml:uri />
        </dev:type>
        <dev:defaultValue>None</dev:defaultValue>
      </command:parameter>
      <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
        <maml:name>system</maml:name>
        <maml:description>
          <maml:para>The system prompt, this is a string, you can use it to define the role you want it be, for example, "You are a chatbot, please answer the user's question according to the user's language." If you provide a file path to this parameter, we will read the file as the system prompt. You can also specify a url to this parameter, we will read the url as the system prompt. You can read the prompt from a library (https://github.com/code365opensource/promptlibrary), by use "lib:xxxxx" as the prompt, for example, "lib:fitness".</maml:para>
        </maml:description>
        <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
        <dev:type>
          <maml:name>String</maml:name>
          <maml:uri />
        </dev:type>
        <dev:defaultValue>You are a chatbot, please answer the user's question according to the user's language.</dev:defaultValue>
      </command:parameter>
      <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
        <maml:name>prompt</maml:name>
        <maml:description>
          <maml:para>If you want to get result immediately, you can use this parameter to define the prompt. It will not start the chat conversation. If you provide a file path to this parameter, we will read the file as the prompt. You can also specify a url to this parameter, we will read the url as the prompt. You can read the prompt from a library (https://github.com/code365opensource/promptlibrary), by use "lib:xxxxx" as the prompt, for example, "lib:fitness".</maml:para>
        </maml:description>
        <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
        <dev:type>
          <maml:name>String</maml:name>
          <maml:uri />
        </dev:type>
        <dev:defaultValue>None</dev:defaultValue>
      </command:parameter>
      <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
        <maml:name>config</maml:name>
        <maml:description>
          <maml:para>The dynamic settings for the API call, it can meet all the requirement for each model. please pass a custom object to this parameter, like @{temperature=1;max_tokens=1024}.</maml:para>
        </maml:description>
        <command:parameterValue required="true" variableLength="false">PSObject</command:parameterValue>
        <dev:type>
          <maml:name>PSObject</maml:name>
          <maml:uri />
        </dev:type>
        <dev:defaultValue>None</dev:defaultValue>
      </command:parameter>
      <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="env">
        <maml:name>environment</maml:name>
        <maml:description>
          <maml:para>The environment name, if you use Azure OpenAI service, you can use this parameter to define the environment name, it will be used to get the API key, model and endpoint from environment variable. If the environment is not exist, it will use the default environment.  You can use env as the alias of this parameter.</maml:para>
        </maml:description>
        <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
        <dev:type>
          <maml:name>String</maml:name>
          <maml:uri />
        </dev:type>
        <dev:defaultValue>None</dev:defaultValue>
      </command:parameter>
      <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
        <maml:name>api_version</maml:name>
        <maml:description>
          <maml:para>The api version, if you use Azure OpenAI service, you can use this parameter to define the api version, the default value is 2023-09-01-preview.</maml:para>
        </maml:description>
        <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
        <dev:type>
          <maml:name>String</maml:name>
          <maml:uri />
        </dev:type>
        <dev:defaultValue>2023-09-01-preview</dev:defaultValue>
      </command:parameter>
      <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="out">
        <maml:name>outFile</maml:name>
        <maml:description>
          <maml:para>If you want to save the result to a file, you can use this parameter to set the file path. You can also use "out" as the alias.</maml:para>
        </maml:description>
        <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
        <dev:type>
          <maml:name>String</maml:name>
          <maml:uri />
        </dev:type>
        <dev:defaultValue>None</dev:defaultValue>
      </command:parameter>
      <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
        <maml:name>json</maml:name>
        <maml:description>
          <maml:para>Send the response in json format.</maml:para>
        </maml:description>
        <command:parameterValue required="false" variableLength="false">SwitchParameter</command:parameterValue>
        <dev:type>
          <maml:name>SwitchParameter</maml:name>
          <maml:uri />
        </dev:type>
        <dev:defaultValue>False</dev:defaultValue>
      </command:parameter>
      <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
        <maml:name>context</maml:name>
        <maml:description>
          <maml:para>If you want to pass some dymamic value to the prompt, you can use the context parameter here. It can be anything, you just specify a custom powershell object here. You define the variables in the system prompt or user prompt by using {{you_variable_name}} syntext, and then pass the data to the context parameter, like @{you_variable_name="your value"}. if there are multiple variables, you can use @{variable1="value1";variable2="value2"}.</maml:para>
        </maml:description>
        <command:parameterValue required="true" variableLength="false">PSObject</command:parameterValue>
        <dev:type>
          <maml:name>PSObject</maml:name>
          <maml:uri />
        </dev:type>
        <dev:defaultValue>None</dev:defaultValue>
      </command:parameter>
    </command:parameters>
    <command:inputTypes />
    <command:returnValues>
      <command:returnValue>
        <dev:type>
          <maml:name>System.String, the completion result.</maml:name>
        </dev:type>
        <maml:description>
          <maml:para></maml:para>
        </maml:description>
      </command:returnValue>
    </command:returnValues>
    <maml:alertSet>
      <maml:alert>
        <maml:para></maml:para>
      </maml:alert>
    </maml:alertSet>
    <command:examples>
      <command:example>
        <maml:title>-------------------------- EXAMPLE 1 --------------------------</maml:title>
        <dev:code>New-ChatGPTConversation</dev:code>
        <dev:remarks>
          <maml:para>Create a new ChatGPT conversation, use OpenAI service with all the default settings.</maml:para>
        </dev:remarks>
      </command:example>
      <command:example>
        <maml:title>-------------------------- EXAMPLE 2 --------------------------</maml:title>
        <dev:code>New-ChatGPTConverstaion -azure</dev:code>
        <dev:remarks>
          <maml:para>Create a new ChatGPT conversation, use Azure OpenAI service with all the default settings.</maml:para>
        </dev:remarks>
      </command:example>
      <command:example>
        <maml:title>-------------------------- EXAMPLE 3 --------------------------</maml:title>
        <dev:code>chat -azure</dev:code>
        <dev:remarks>
          <maml:para>Create a new ChatGPT conversation by cmdlet's alias(chat), use Azure OpenAI service with all the default settings.</maml:para>
        </dev:remarks>
      </command:example>
      <command:example>
        <maml:title>-------------------------- EXAMPLE 4 --------------------------</maml:title>
        <dev:code>New-ChatGPTConversation -api_key "your API key" -model "your model name"</dev:code>
        <dev:remarks>
          <maml:para>Create a new ChatGPT conversation, use OpenAI service with your API key and model name.</maml:para>
        </dev:remarks>
      </command:example>
      <command:example>
        <maml:title>-------------------------- EXAMPLE 5 --------------------------</maml:title>
        <dev:code>New-ChatGPTConversation -api_key "your API key" -model "your deployment name" -azure</dev:code>
        <dev:remarks>
          <maml:para>Create a new ChatGPT conversation, use Azure OpenAI service with your API key and deployment name.</maml:para>
        </dev:remarks>
      </command:example>
      <command:example>
        <maml:title>-------------------------- EXAMPLE 6 --------------------------</maml:title>
        <dev:code>New-ChatGPTConversation -api_key "your API key" -model "your deployment name" -azure -system "You are a chatbot, please answer the user's question according to the user's language."</dev:code>
        <dev:remarks>
          <maml:para>Create a new ChatGPT conversation, use Azure OpenAI service with your API key and deployment name, and define the system prompt.</maml:para>
        </dev:remarks>
      </command:example>
      <command:example>
        <maml:title>-------------------------- EXAMPLE 7 --------------------------</maml:title>
        <dev:code>New-ChatGPTConversation -api_key "your API key" -model "your deployment name" -azure -system "You are a chatbot, please answer the user's question according to the user's language." -endpoint "https://api.openai.com/v1/completions"</dev:code>
        <dev:remarks>
          <maml:para>Create a new ChatGPT conversation, use Azure OpenAI service with your API key and model id, and define the system prompt and endpoint.</maml:para>
        </dev:remarks>
      </command:example>
      <command:example>
        <maml:title>-------------------------- EXAMPLE 8 --------------------------</maml:title>
        <dev:code>chat -azure -system "You are a chatbot, please answer the user's question according to the user's language." -env "sweden"</dev:code>
        <dev:remarks>
          <maml:para>Create a new ChatGPT conversation by cmdlet's alias(chat), use Azure OpenAI service with the API key, model and endpoint defined in environment variable OPENAI_API_KEY_AZURE_SWEDEN, OPENAI_CHAT_DEPLOYMENT_AZURE_SWEDEN and OPENAI_ENDPOINT_AZURE_SWEDEN.</maml:para>
        </dev:remarks>
      </command:example>
      <command:example>
        <maml:title>-------------------------- EXAMPLE 9 --------------------------</maml:title>
        <dev:code>chat -azure -api_version "2021-09-01-preview"</dev:code>
        <dev:remarks>
          <maml:para>Create a new ChatGPT conversation by cmdlet's alias(chat), use Azure OpenAI service with the api version 2021-09-01-preview.</maml:para>
        </dev:remarks>
      </command:example>
      <command:example>
        <maml:title>-------------------------- EXAMPLE 10 --------------------------</maml:title>
        <dev:code>gpt -azure -prompt "why people smile"</dev:code>
        <dev:remarks>
          <maml:para>Create a new ChatGPT conversation by cmdlet's alias(gpt), use Azure OpenAI service with the prompt.</maml:para>
        </dev:remarks>
      </command:example>
      <command:example>
        <maml:title>-------------------------- EXAMPLE 11 --------------------------</maml:title>
        <dev:code>"why people smile" | gpt -azure</dev:code>
        <dev:remarks>
          <maml:para>Create a new ChatGPT conversation by cmdlet's alias(gpt), use Azure OpenAI service with the prompt from pipeline.</maml:para>
        </dev:remarks>
      </command:example>
      <command:example>
        <maml:title>-------------------------- EXAMPLE 12 --------------------------</maml:title>
        <dev:code>gpt -azure -prompt "c:\temp\prompt.txt"</dev:code>
        <dev:remarks>
          <maml:para>Create a new ChatGPT conversation by cmdlet's alias(gpt), use Azure OpenAI service with the prompt from file.</maml:para>
        </dev:remarks>
      </command:example>
      <command:example>
        <maml:title>-------------------------- EXAMPLE 13 --------------------------</maml:title>
        <dev:code>gpt -azure -prompt "c:\temp\prompt.txt" -context @{variable1="value1";variable2="value2"}</dev:code>
        <dev:remarks>
          <maml:para>Create a new ChatGPT conversation by cmdlet's alias(gpt), use Azure OpenAI service with the prompt from file, pass some data to the prompt.</maml:para>
        </dev:remarks>
      </command:example>
      <command:example>
        <maml:title>-------------------------- EXAMPLE 14 --------------------------</maml:title>
        <dev:code>gpt -azure -system "c:\temp\system.txt" -prompt "c:\temp\prompt.txt"</dev:code>
        <dev:remarks>
          <maml:para>Create a new ChatGPT conversation by cmdlet's alias(gpt), use Azure OpenAI service with the system prompt and prompt from file.</maml:para>
        </dev:remarks>
      </command:example>
      <command:example>
        <maml:title>-------------------------- EXAMPLE 15 --------------------------</maml:title>
        <dev:code>gpt -azure -system "c:\temp\system.txt" -prompt "c:\temp\prompt.txt" -outFile "c:\temp\result.txt"</dev:code>
        <dev:remarks>
          <maml:para>Create a new ChatGPT conversation by cmdlet's alias(gpt), use Azure OpenAI service with the system prompt and prompt from file, then save the result to a file.</maml:para>
        </dev:remarks>
      </command:example>
      <command:example>
        <maml:title>-------------------------- EXAMPLE 16 --------------------------</maml:title>
        <dev:code>gpt -azure -system "c:\temp\system.txt" -prompt "c:\temp\prompt.txt" -config @{temperature=1;max_tokens=1024}</dev:code>
        <dev:remarks>
          <maml:para>Create a new ChatGPT conversation by cmdlet's alias(gpt), use Azure OpenAI service with the system prompt and prompt from file and your customized settings.</maml:para>
        </dev:remarks>
      </command:example>
      <command:example>
        <maml:title>-------------------------- EXAMPLE 17 --------------------------</maml:title>
        <dev:code>chat -local -model "llama3"</dev:code>
        <dev:remarks>
          <maml:para>Create a new ChatGPT conversation by using local LLMs, for example, the llama3. The default endpoint is http://localhost:11434/v1/chat/completions. You can modify this endpoint as well.</maml:para>
        </dev:remarks>
      </command:example>
    </command:examples>
    <command:relatedLinks>
      <maml:navigationLink>
        <maml:linkText>https://github.com/chenxizhang/openai-powershell</maml:linkText>
        <maml:uri>https://github.com/chenxizhang/openai-powershell</maml:uri>
      </maml:navigationLink>
    </command:relatedLinks>
  </command:command>
  <command:command xmlns:maml="http://schemas.microsoft.com/maml/2004/10" xmlns:command="http://schemas.microsoft.com/maml/dev/command/2004/10" xmlns:dev="http://schemas.microsoft.com/maml/dev/2004/10" xmlns:MSHelp="http://msdn.microsoft.com/mshelp">
    <command:details>
      <command:name>New-ImageGeneration</command:name>
      <command:verb>New</command:verb>
      <command:noun>ImageGeneration</command:noun>
      <maml:description>
        <maml:para>Generate image from prompt, using DALL-e-3 model.</maml:para>
      </maml:description>
    </command:details>
    <maml:description>
      <maml:para>Generate image from prompt, using DALL-e-3 model. The image size can be 1024x1024, 1792x1024, 1024x1792.</maml:para>
    </maml:description>
    <command:syntax>
      <command:syntaxItem>
        <maml:name>New-ImageGeneration</maml:name>
        <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
          <maml:name>prompt</maml:name>
          <maml:description>
            <maml:para>The prompt to generate image, this is required, and it can pass from pipeline. If you want to use a file as prompt, you can specify the file path here. You can also specify a url as prompt, we will read the url as prompt. You can read the prompt from a library (https://github.com/code365opensource/promptlibrary), by use "lib:xxxxx" as the prompt, for example, "lib:fitness".</maml:para>
          </maml:description>
          <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
          <dev:type>
            <maml:name>String</maml:name>
            <maml:uri />
          </dev:type>
          <dev:defaultValue>None</dev:defaultValue>
        </command:parameter>
        <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
          <maml:name>api_key</maml:name>
          <maml:description>
            <maml:para>The API key to access OpenAI service, if not specified, the API key will be read from environment variable OPENAI_API_KEY. if you use Azure OpenAI service, you can specify the API key by environment variable OPENAI_API_KEY_AZURE or OPENAI_API_KEY_AZURE_&lt;environment&gt;, the &lt;environment&gt; can be any names you want, for example, OPENAI_API_KEY_AZURE_DEV, OPENAI_API_KEY_AZURE_PROD, OPENAI_API_KEY_AZURE_TEST, etc.</maml:para>
          </maml:description>
          <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
          <dev:type>
            <maml:name>String</maml:name>
            <maml:uri />
          </dev:type>
          <dev:defaultValue>None</dev:defaultValue>
        </command:parameter>
        <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
          <maml:name>endpoint</maml:name>
          <maml:description>
            <maml:para>The endpoint to access OpenAI service, if not specified, the endpoint will be read from environment variable OPENAI_ENDPOINT. if you use Azure OpenAI service, you can specify the endpoint by environment variable OPENAI_ENDPOINT_AZURE or OPENAI_ENDPOINT_AZURE_&lt;environment&gt;, the &lt;environment&gt; can be any names you want, for example, OPENAI_ENDPOINT_AZURE_DEV, OPENAI_ENDPOINT_AZURE_PROD, OPENAI_ENDPOINT_AZURE_TEST, etc.</maml:para>
          </maml:description>
          <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
          <dev:type>
            <maml:name>String</maml:name>
            <maml:uri />
          </dev:type>
          <dev:defaultValue>None</dev:defaultValue>
        </command:parameter>
        <command:parameter required="true" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
          <maml:name>azure</maml:name>
          <maml:description>
            <maml:para>Use Azure OpenAI service, if specified, the API key and endpoint will be read from environment variable OPENAI_API_KEY_AZURE or OPENAI_API_KEY_AZURE_&lt;environment&gt;, the &lt;environment&gt; can be any names you want, for example, OPENAI_API_KEY_AZURE_DEV, OPENAI_API_KEY_AZURE_PROD, OPENAI_API_KEY_AZURE_TEST, etc. and OPENAI_ENDPOINT_AZURE or OPENAI_ENDPOINT_AZURE_&lt;environment&gt;.</maml:para>
          </maml:description>
          <dev:type>
            <maml:name>SwitchParameter</maml:name>
            <maml:uri />
          </dev:type>
          <dev:defaultValue>False</dev:defaultValue>
        </command:parameter>
        <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
          <maml:name>size</maml:name>
          <maml:description>
            <maml:para>The size of the image to generate, the value can be small (1024x1024), medium(1792x1024), large(1024x1792), the default is small.</maml:para>
          </maml:description>
          <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
          <dev:type>
            <maml:name>String</maml:name>
            <maml:uri />
          </dev:type>
          <dev:defaultValue>Small</dev:defaultValue>
        </command:parameter>
        <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="out">
          <maml:name>outfolder</maml:name>
          <maml:description>
            <maml:para>The folder to save the generated image, default is current folder. You can use out as the alias of this parameter.</maml:para>
          </maml:description>
          <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
          <dev:type>
            <maml:name>String</maml:name>
            <maml:uri />
          </dev:type>
          <dev:defaultValue>.</dev:defaultValue>
        </command:parameter>
        <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="env">
          <maml:name>environment</maml:name>
          <maml:description>
            <maml:para>The environment name, if you use Azure OpenAI service, you can specify the environment by this parameter, the environment name can be any names you want, for example, dev, prod, test, etc, the environment name will be used to read the API key and endpoint from environment variable, for example, OPENAI_API_KEY_AZURE_DEV, OPENAI_ENDPOINT_AZURE_DEV, etc. You can use env as the alias of this parameter.</maml:para>
          </maml:description>
          <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
          <dev:type>
            <maml:name>String</maml:name>
            <maml:uri />
          </dev:type>
          <dev:defaultValue>None</dev:defaultValue>
        </command:parameter>
      </command:syntaxItem>
    </command:syntax>
    <command:parameters>
      <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
        <maml:name>prompt</maml:name>
        <maml:description>
          <maml:para>The prompt to generate image, this is required, and it can pass from pipeline. If you want to use a file as prompt, you can specify the file path here. You can also specify a url as prompt, we will read the url as prompt. You can read the prompt from a library (https://github.com/code365opensource/promptlibrary), by use "lib:xxxxx" as the prompt, for example, "lib:fitness".</maml:para>
        </maml:description>
        <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
        <dev:type>
          <maml:name>String</maml:name>
          <maml:uri />
        </dev:type>
        <dev:defaultValue>None</dev:defaultValue>
      </command:parameter>
      <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
        <maml:name>api_key</maml:name>
        <maml:description>
          <maml:para>The API key to access OpenAI service, if not specified, the API key will be read from environment variable OPENAI_API_KEY. if you use Azure OpenAI service, you can specify the API key by environment variable OPENAI_API_KEY_AZURE or OPENAI_API_KEY_AZURE_&lt;environment&gt;, the &lt;environment&gt; can be any names you want, for example, OPENAI_API_KEY_AZURE_DEV, OPENAI_API_KEY_AZURE_PROD, OPENAI_API_KEY_AZURE_TEST, etc.</maml:para>
        </maml:description>
        <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
        <dev:type>
          <maml:name>String</maml:name>
          <maml:uri />
        </dev:type>
        <dev:defaultValue>None</dev:defaultValue>
      </command:parameter>
      <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
        <maml:name>endpoint</maml:name>
        <maml:description>
          <maml:para>The endpoint to access OpenAI service, if not specified, the endpoint will be read from environment variable OPENAI_ENDPOINT. if you use Azure OpenAI service, you can specify the endpoint by environment variable OPENAI_ENDPOINT_AZURE or OPENAI_ENDPOINT_AZURE_&lt;environment&gt;, the &lt;environment&gt; can be any names you want, for example, OPENAI_ENDPOINT_AZURE_DEV, OPENAI_ENDPOINT_AZURE_PROD, OPENAI_ENDPOINT_AZURE_TEST, etc.</maml:para>
        </maml:description>
        <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
        <dev:type>
          <maml:name>String</maml:name>
          <maml:uri />
        </dev:type>
        <dev:defaultValue>None</dev:defaultValue>
      </command:parameter>
      <command:parameter required="true" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
        <maml:name>azure</maml:name>
        <maml:description>
          <maml:para>Use Azure OpenAI service, if specified, the API key and endpoint will be read from environment variable OPENAI_API_KEY_AZURE or OPENAI_API_KEY_AZURE_&lt;environment&gt;, the &lt;environment&gt; can be any names you want, for example, OPENAI_API_KEY_AZURE_DEV, OPENAI_API_KEY_AZURE_PROD, OPENAI_API_KEY_AZURE_TEST, etc. and OPENAI_ENDPOINT_AZURE or OPENAI_ENDPOINT_AZURE_&lt;environment&gt;.</maml:para>
        </maml:description>
        <command:parameterValue required="false" variableLength="false">SwitchParameter</command:parameterValue>
        <dev:type>
          <maml:name>SwitchParameter</maml:name>
          <maml:uri />
        </dev:type>
        <dev:defaultValue>False</dev:defaultValue>
      </command:parameter>
      <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="none">
        <maml:name>size</maml:name>
        <maml:description>
          <maml:para>The size of the image to generate, the value can be small (1024x1024), medium(1792x1024), large(1024x1792), the default is small.</maml:para>
        </maml:description>
        <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
        <dev:type>
          <maml:name>String</maml:name>
          <maml:uri />
        </dev:type>
        <dev:defaultValue>Small</dev:defaultValue>
      </command:parameter>
      <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="out">
        <maml:name>outfolder</maml:name>
        <maml:description>
          <maml:para>The folder to save the generated image, default is current folder. You can use out as the alias of this parameter.</maml:para>
        </maml:description>
        <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
        <dev:type>
          <maml:name>String</maml:name>
          <maml:uri />
        </dev:type>
        <dev:defaultValue>.</dev:defaultValue>
      </command:parameter>
      <command:parameter required="false" variableLength="true" globbing="false" pipelineInput="False" position="named" aliases="env">
        <maml:name>environment</maml:name>
        <maml:description>
          <maml:para>The environment name, if you use Azure OpenAI service, you can specify the environment by this parameter, the environment name can be any names you want, for example, dev, prod, test, etc, the environment name will be used to read the API key and endpoint from environment variable, for example, OPENAI_API_KEY_AZURE_DEV, OPENAI_ENDPOINT_AZURE_DEV, etc. You can use env as the alias of this parameter.</maml:para>
        </maml:description>
        <command:parameterValue required="true" variableLength="false">String</command:parameterValue>
        <dev:type>
          <maml:name>String</maml:name>
          <maml:uri />
        </dev:type>
        <dev:defaultValue>None</dev:defaultValue>
      </command:parameter>
    </command:parameters>
    <command:inputTypes />
    <command:returnValues>
      <command:returnValue>
        <dev:type>
          <maml:name>System.String, the file path of the generated image.</maml:name>
        </dev:type>
        <maml:description>
          <maml:para></maml:para>
        </maml:description>
      </command:returnValue>
    </command:returnValues>
    <maml:alertSet>
      <maml:alert>
        <maml:para></maml:para>
      </maml:alert>
    </maml:alertSet>
    <command:examples>
      <command:example>
        <maml:title>-------------------------- EXAMPLE 1 --------------------------</maml:title>
        <dev:code>New-ImageGeneration -prompt "A painting of a cat sitting on a chair"</dev:code>
        <dev:remarks>
          <maml:para>Use dall-e-3 model to generate image, the image size is 1024x1024, the generated image will be saved to current folder.</maml:para>
        </dev:remarks>
      </command:example>
      <command:example>
        <maml:title>-------------------------- EXAMPLE 2 --------------------------</maml:title>
        <dev:code>image -prompt "A painting of a cat sitting on a chair"</dev:code>
        <dev:remarks>
          <maml:para>Use the alias (image) to generate image, the image size is 1024x1024, the generated image will be saved to current folder.</maml:para>
        </dev:remarks>
      </command:example>
      <command:example>
        <maml:title>-------------------------- EXAMPLE 3 --------------------------</maml:title>
        <dev:code>"A painting of a cat sitting on a chair" | New-ImageGeneration</dev:code>
        <dev:remarks>
          <maml:para>Pass the prompt from pipeline, the image size is 1024x1024, the generated image will be saved to current folder.</maml:para>
        </dev:remarks>
      </command:example>
      <command:example>
        <maml:title>-------------------------- EXAMPLE 4 --------------------------</maml:title>
        <dev:code>New-ImageGeneration -prompt "A painting of a cat sitting on a chair" -size medium -outfolder "c:\temp" -api_key "your API key" -endpoint "your endpoint"</dev:code>
        <dev:remarks>
          <maml:para>Use dall-e-3 model to generate image, the image size is 1792x1024, the generated image will be saved to c:\temp folder, use your own API key and endpoint.</maml:para>
        </dev:remarks>
      </command:example>
      <command:example>
        <maml:title>-------------------------- EXAMPLE 5 --------------------------</maml:title>
        <dev:code>New-ImageGeneration -prompt "A painting of a cat sitting on a chair" -size small -outfolder "c:\temp" -azure</dev:code>
        <dev:remarks>
          <maml:para>Use dall-e-3 model to generate image, the image size is 1024x1024, the generated image will be saved to c:\temp folder, use Azure OpenAI service.</maml:para>
        </dev:remarks>
      </command:example>
      <command:example>
        <maml:title>-------------------------- EXAMPLE 6 --------------------------</maml:title>
        <dev:code>New-ImageGeneration -prompt "A painting of a cat sitting on a chair" -size small -outfolder "c:\temp" -azure -environment "dev"</dev:code>
        <dev:remarks>
          <maml:para>Use dall-e-3 model to generate image, the image size is 1024x1024, the generated image will be saved to c:\temp folder, use Azure OpenAI service, read API key and endpoint from environment variable OPENAI_API_KEY_AZURE_DEV and OPENAI_ENDPOINT_AZURE_DEV.</maml:para>
        </dev:remarks>
      </command:example>
      <command:example>
        <maml:title>-------------------------- EXAMPLE 7 --------------------------</maml:title>
        <dev:code>New-ImageGeneration -outfolder "c:\temp" -azure -prompt "c:\temp\prompt.txt"</dev:code>
        <dev:remarks>
          <maml:para>Use dall-e-3 model to generate image, the image size is 1024x1024, the generated image will be saved to c:\temp folder, use Azure OpenAI service, and use prompt from file c:\temp\prompt.txt</maml:para>
        </dev:remarks>
      </command:example>
    </command:examples>
    <command:relatedLinks>
      <maml:navigationLink>
        <maml:linkText>https://github.com/chenxizhang/openai-powershell</maml:linkText>
        <maml:uri>https://github.com/chenxizhang/openai-powershell</maml:uri>
      </maml:navigationLink>
    </command:relatedLinks>
  </command:command>
</helpItems>